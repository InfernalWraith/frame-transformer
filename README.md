# frame-transformer

This fork is mainly a research fork and will change frequently and there is like zeo focus on being user friendly. I am happy to answer any and all questions however.

It seems as though the below model doesn't extrapolate very well. I have finally built my second workstation, so I will be launching an experiment tonight to investigate why that is. First, I will be training a thin frame transformer using a sliding window to extract 256 frames from a larger 512 frame spectrogram. I will also train a full frame transformer. I suspect the sliding cropsize augmentation primes the attention mechanism to look around more readily, but I'm not entirely sure so it will be interesting to see the results. Will begin building a third workstation next, goal is to slowly expand compute capabilities in order to scale project up and maximize quality while also providing a stepping stone for a musical stable diffusion.

Currently training the model in frame_tranformer_thin.py. This variant uses the single channel technique seen in the original fork and then uses a single channel transformer encoder for the encoder sequence in the u-net and single channel transformer decoders for the decoding sequence; it uses squared ReLU as in the Primer paper however the attention mechanism does not include convolutions given the convolutional backbone for the transformer. So far it seems to have surpassed all my previous versions at only 200k optimization steps. After this one is finished, I will train a larger model using the full frame transformer setup. I expect it to perform a bit better, but it will also be far slower so there are trade-offs there (this one can also have more context which should help). Here is a checkpoint for the thin frame transformer at epoch 6. This is trained on about 40-45 days of instrumental music that is randomly mixed with 1500 vocal tracks; this uses the voxaug2 dataset which ensures that no training example is repeated until every combiniation of training data has been seen which means that this has never seen the same training example more than once. This checkpoint uses the default hyperparameters in inference_thin.py. https://mega.nz/file/rw4RkBrD#2MbnMFujw8hGqsTZxaW16FWwi-m-GW4vqtask45wJBM

I did notice one weird thing about the above when I tried it after only a single epoch. It almost seemed as though more context didn't help this time, which makes me curious. I regenerated my dataset to use a base cropsize of 256 to lighten the load on my SSDs. I'm beginning to wonder if using the same 2x1024x256 means that the attention mechanism will be less primed to look around and will be more focused on specific areas; previously when using a sliding window extracted from a larger spectrogram increasing context meant higher quality. I will be testing the current version in about 3 hours after the current epoch and see if context helps or hurts; if it hurts, I will add back in the chunking code and stitch together two 256 frame spectrograms into 512 frames and then randomly extract a 256 frame subset from those 512 frames in order to shift each instrumental example around a bit and add another layer of augmentation.

The overall architecture I refer to as a frame transformer. It is a u-net that uses a stride of (2,1) for its downsampling convolutions to preserve the temporal dimension while still using 3x3 kernels. Each layer in the u-net is preceded by a frame transformer encoder. Each encoder/decoder/output is defined as frame_encoder(frame_transformer_encoder(x)), frame_decoder(frame_transformer_encoder(x), skip), and then conv2d(frame_transformer_encoder(x)) for the output where the conv2d uses a 1x1 convolution (this just uses matrix multiplication in the code but its basically just a 1x1 conv2d with no bias). Frame transformer encoders utilize multichannel multihead attention to carry out multihead attention across each channel in parallel where its calculating attention scores between the frames of individual channels. for the QKVO projections, it uses multichannel linear layers. These are my implementation of parallel linear layers with a depthwise component; it uses batched matrix multiplication to take advantage of the GPU and is significantly faster than grouped 1d convolutions. The feedforward component of the frame transformer encoder also uses multichannel linear layers; this means that the frame transformer at each level consists of parallel transformer modules where each channel can be considered its own transformer encoder. The results of the frame transformer encoder are then concatenated with the input to double the number of channels where there is an equal number of feature maps and attention maps. This uses squared ReLU as in the primer architecture; the attention mechanism no longer uses convolutions in order to cut down on the time it takes to train the model. 

There are now two versions of voxaug; the first one randomly selects vocal track to mix with instrumentals. Voxaug2 will cycle through all vocals for each instrumental spectrogram, requiring N number of training passes to complete an epoch where N is the number of vocal spectrograms in the dataset. For example with my dataset, it would take 54,393 epochs for each instrumental spectrogram to see each vocal spectrogram (and with each epoch taking 12 hours it safe to say I won't be training that far without getting more compute... that being said, I think musdb18 should work pretty well with this although even then it would likely take a very long time). I also have a new optimization I'm testing. This takes the audio with vocals and downsamples it via nearest neighbor, and then afterward scales the mask back up to the original size and multiplies that with the original pre-scaled spectrogram with vocals. Speeds things up about 2x while the loss appears to be in lockstep, at 70k steps this version is at the same validation loss as the non-scaled version.

Here is a checkpoint of an older version:
Not sure if this means that the attention maps are important or if the u-net channels are less important or if maybe both are true to an extent, will be testing this eventually. A corrector model at epoch 21 is here: https://mega.nz/file/ytYAXBAC#pSmIEg45Y00-N2pboW_hDZZK5OzAX4vpQCr0mu0zWNk, and a vocal remover model at epoch 59 is here: https://mega.nz/file/zp4ATDIA#uyX7e3mg4f4kujJJSj_7hPZRgOxKvn9-WPJ5lAndxGY. You will need to use the code from this commit: https://github.com/carperbr/frame-transformer/tree/61d81c610fc4ec5fb8170003e3096dd4e8ff5ee2 in order to use them. They work well for most bands, however there were songs with vocals in the instrumental section which has caused the vocal remover to not remove specific kinds of vocals. Due to the attention mechanism this is dependent on context so won't really cause issues outside of First Fragment and similar vocal styles, but even then will work well most of the time. If you try to remove vocals from Solus by First Fragment, however, it will not work. It was taught to leave vocals in that song only lol. New model should only take a couple days to train, error corrector is trained using synthetic data so won't need retraining.

## References
- [1] Jansson et al., "Singing Voice Separation with Deep U-Net Convolutional Networks", https://ismir2017.smcnus.org/wp-content/uploads/2017/10/171_Paper.pdf
- [2] Takahashi et al., "Multi-scale Multi-band DenseNets for Audio Source Separation", https://arxiv.org/pdf/1706.09588.pdf
- [3] Takahashi et al., "MMDENSELSTM: AN EFFICIENT COMBINATION OF CONVOLUTIONAL AND RECURRENT NEURAL NETWORKS FOR AUDIO SOURCE SEPARATION", https://arxiv.org/pdf/1805.02410.pdf
- [4] Liutkus et al., "The 2016 Signal Separation Evaluation Campaign", Latent Variable Analysis and Signal Separation - 12th International Conference
- [5] Vaswani et al., "Attention Is All You Need", https://arxiv.org/pdf/1706.03762.pdf
- [6] So et al., "Primer: Searching for Efficient Transformers for Language Modeling", https://arxiv.org/pdf/2109.08668v2.pdf
- [7] Su et al., "RoFormer: Enhanced Transformer with Rotary Position Embedding", https://arxiv.org/abs/2104.09864
- [9] Asiedu et all., "Decoder Denoising Pretraining for Semantic Segmentation", https://arxiv.org/abs/2205.11423
