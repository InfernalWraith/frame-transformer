# vocal-remover

This is a deep-learning-based tool to extract instrumental track from your songs.

Keep making changes and expanding my dataset, however I think I've pushed my hardware to its limits now, both storage and compute. My current plans for this fork are as follows: I will first run a supervised training session using the frame primer unet variant. The current version has 108,229,338 parameters. The dataset consists of two parts, a instrumental/mix pair collection and a vocal augmentation library. There are 460 vocal mix/instrumental pairs, and as far as the vocal augmentation dataset goes I have 1,297 vocal tracks and 28.23 days of instrumental spectrograms (no idea how many songs at this point) which are randomly combined to create new training data to satisfy the transformers need for large amounts of data. For pretraining, there is a total of 83.43 days of both instrumental spectrograms and mix spectrograms; the current plan is to have it predict the noise as in the decoder denoising pretraining paper. Not sure if its the dataset or the architecture or both, but one area the models typically excel at and get perfectly where most tools I've seen struggle is fretless bass and understanding what is actually vocals and what isn't. Unfortunately I don't think I can shrae the dataset itself, so checkpoints will have to suffice with which people can finetune.

As far as getting a checkpoint of the model, I have started training my final instance for the model will upload it in stages. The first will be one trained on clips of 10 seconds of audio, the second 20 seconds of audio, and the last will be trained on 40 seconds of audio. After this final supervised stage, I will shift to my second attempt at pretraining, this time using the decoider denoising pretraining procedure (however applied to the entire architecture). This one will take a while given the size of my dataset, however I will likely only finetune it on a smaller subset of data for testing purposes, namely the 460 song pair subset of the main vocal remover dataset.

Current version that I'm training is using both encoders and decoders; there isn't a huge difference when using encoders/deocders vs just decoders, pretty sure that was just due to random seed. The benefit of using encoders, however, is that they can be frozen on the finetuning phase which speeds up the training process quite drastically in that stage. The architecture is the frame primer; it is still using a stride of Nx1, however I do have it using 3x3 kernels now. It doesn't really seem to make much of a difference using 3x3 kernels vs 3x1, will need to look into that further however for now I'm just shifting back to the usual 3x3 kernels. I still use the stride of 2x1 to limit the need to increase channels quite so drastically so as to save memory consumption. The current instance of the architecture is using 5 encoders/decoders, and 2 frame primer encoders/decoders at each step. The frame primer modules use residual attention connections as well as relative positional encoding, and make use of the multi-dconv-head attention variant of multiheaded attention from the primer architecture. One deviation I've made is the use of GELU instead of rectified RELU in the transformer block.

## References
- [1] Jansson et al., "Singing Voice Separation with Deep U-Net Convolutional Networks", https://ismir2017.smcnus.org/wp-content/uploads/2017/10/171_Paper.pdf
- [2] Takahashi et al., "Multi-scale Multi-band DenseNets for Audio Source Separation", https://arxiv.org/pdf/1706.09588.pdf
- [3] Takahashi et al., "MMDENSELSTM: AN EFFICIENT COMBINATION OF CONVOLUTIONAL AND RECURRENT NEURAL NETWORKS FOR AUDIO SOURCE SEPARATION", https://arxiv.org/pdf/1805.02410.pdf
- [4] Liutkus et al., "The 2016 Signal Separation Evaluation Campaign", Latent Variable Analysis and Signal Separation - 12th International Conference
- [5] Vaswani et al., "Attention Is All You Need", https://arxiv.org/pdf/1706.03762.pdf
- [6] So et al., "Primer: Searching for Efficient Transformers for Language Modeling", https://arxiv.org/pdf/2109.08668v2.pdf
- [7] Huang et al., "Music Transformer: Generating Music with Long-Term Structure", https://arxiv.org/pdf/1809.04281.pdf
- [8] He et al., "RealFormer: Transformer Likes Residual Attention", https://arxiv.org/pdf/2012.11747.pdf
- [9] Asiedu et all., "Decoder Denoising Pretraining for Semantic Segmentation", https://arxiv.org/abs/2205.11423
