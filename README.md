# vocal-remover

This is a deep-learning-based tool to extract instrumental track from your songs.

This is a variation of tsurumeso's vocal remover that I've been tinkering with for a while that I'm calling a frame transformer. The goal of this fork is to find a meaningful way to use the evolved transformer architecture for track separation. This isn't a very user-friendly version just yet, has a lot of stuff that is specific to my dataset and environment; will take at least 12gb of VRAM to train this given the settings in the code. Will pretty this up over time.

This version consists of only a single u-net. The u-net uses frame convolutions with 3x1 kernels and a stride of 2x1 to embed frequency information into the networks feature maps and preserve the temporal axis. The current architecture is decoder only; before each decoder and the final out projection there is a sequence of frame transformer decoders. The memory for the frame transformers is the skip connection from the corresponding stage. Each frame transformer decoder makes use of multiband frame attention, which computes attention between frames within a set number of frequency bands - the encoded frequency bins are the embedding dimensions which are split across the attention heads which I refer to as attention bands since its the most accurate name in this context. This makes use of a post-norm variant of the evolved transformer decoder. Each transformer module in the same stage is connected in a dense fashion which seemed to perform better than just N transformer layers in a row with only a single bottleneck. All information is shared between frames within the transformer decoder modules via the 1d convolutions and the multiband frame attention modules. Comparing this architecture with the same with only 3x1 convolutions and preservation of the temporal axis did far worse, so the boost in accuracy seems to be coming from the transformer module. Should be interesting to try scaling this up however it uses quite a bit of RAM so I haven't had much of a chance; a single u-net with the frame transformer seems to get a lower validation loss on my dataset than the pure convolutional neural network variant of tsurumeso, so a cascaded setup is on my list of things to try once I get my new machine built. However, early tests did point to the amount of data being quite important for making the transformer variant outperform the convolutional variant (which from what I understand is par for the course thus far in other areas using transformers). If anyone has ideas and wants to work with me on this transformer variant please feel free to reach out, my goal is to continue pushing to make this as high quality as possible. I am trying to get an autoregressive version built but haven't had much luck on that front just yet.

I will likely restructure this repo soon and include my Google Cloud dataloader and training script, dockerfile etc etc. My current cloud version streams the data from google cloud storage so is meant to be used with a cluster via distributed data parallel.

Will update this repo with checkpoints once I've converged on a more final architecture (which I believe I have now); I have an RTX 3090 Ti sitting in the corner so I should soon be able to train an even larger version that people can use for inference. My personal dataset consists of 5,249 instrumental songs from many genres, 488 vocal tracks from many genres, and 345 instrumental + mix pairs mainly from metal but with some rap and tracks from MUSDB18 all of which comes out to around 1TB. An example of this architecture after 4 epochs on my dataset is here: https://www.youtube.com/watch?v=bAJ_zUlUcAA, a vocal extraction (warning, the vocals here are screams) is here: https://www.youtube.com/watch?v=Wny0gBz_3Og with instrumental counterpart here: https://www.youtube.com/watch?v=jMVcX9RQCbg

This fork makes use of vocal augmentations. The dataset takes an instrumental path, an optional second instrumental path if you have a dataset split across multiple locations, a vocal path, and a pair path. The dataset's __getitem__ method will check to see if a Y value exists in the npz file; instrumental npzs will be expected to only have an 'X' and a 'c' key defined. If a 'Y' key does not exist, it will treat it as a file that should be augmented with vocals. This will cause the dataset to randomly select a vocal file, augment the vocal spectrogram, and then add the vocal spectrogram to the instrumental track and select the largest of either the instrumental max magnitude, vocal max magnitude, or the max magnitude of that slice. There is also a chance that vocals will be randomly added to pair tracks, so the vocal version will get an extra layer of vocals added to it and a new divisor selected if need be for normalization.

## References
- [1] Jansson et al., "Singing Voice Separation with Deep U-Net Convolutional Networks", https://ismir2017.smcnus.org/wp-content/uploads/2017/10/171_Paper.pdf
- [2] Takahashi et al., "Multi-scale Multi-band DenseNets for Audio Source Separation", https://arxiv.org/pdf/1706.09588.pdf
- [3] Takahashi et al., "MMDENSELSTM: AN EFFICIENT COMBINATION OF CONVOLUTIONAL AND RECURRENT NEURAL NETWORKS FOR AUDIO SOURCE SEPARATION", https://arxiv.org/pdf/1805.02410.pdf
- [4] Liutkus et al., "The 2016 Signal Separation Evaluation Campaign", Latent Variable Analysis and Signal Separation - 12th International Conference
- [5] So, Liang, and Le, "The Evolved Transformer", https://arxiv.org/pdf/1901.11117.pdf
- [6] Huang et al., "Music Transformer: Generating Music with Long-Term Structure", https://arxiv.org/pdf/1809.04281.pdf