# vocal-remover

This is a deep-learning-based tool to extract instrumental track from your songs.

This is a variation of tsurumeso's vocal remover that I've been tinkering with for a while that I'm calling a frame transformer. The goal of this fork is to find a meaningful way to use the evolved transformer architecture for track separation. This isn't a very user-friendly version just yet, has a lot of stuff that is specific to my dataset and environment; will take at least 12gb of VRAM to train this given the settings in the code. Will pretty this up over time.

This version consists of only a single u-net. This u-net includes modified evolved transformer encoders after each downsampling as well as modified evolved transformer decoders before each upsampling.

Will update this repo in the coming days with checkpoints with listed settings. My personal dataset consists of 5,249 instrumental songs from many genres, 488 vocal tracks from many genres, and 345 instrumental + mix pairs mainly from metal but with some rap and tracks from MUSDB18 all of which comes out to around 1TB. An example of this architecture after 4 epochs on my dataset is here: https://www.youtube.com/watch?v=bAJ_zUlUcAA, a vocal extraction (warning, the vocals here are screams) is here: https://www.youtube.com/watch?v=Wny0gBz_3Og with instrumental counterpart here: https://www.youtube.com/watch?v=jMVcX9RQCbg

This architecture consists of two parts: a convolutional portion which uses Nx1 kernels, and a transformer portion that occurs on a bottlenecked representation of the frames convolutional embeddings. The convolutional portion of the architecture serves to encode frequency information without respect to time and uses 3x1 kernels on the encoder and decoder convolutions in order to do so. All communication between frames occurs in the transformer modules which contain various 1d convolutions and 1d depthwise separable convolutions of varying sizes as well as multihead attention in the form of the MultibandFrameAttention module. The current architecture is making use of residual attention layers across all encoding steps and decoding steps allowing the pre-softmax attention scores to flow through the encoder sequence and decoder sequence. Each transformer module in each encoder step and decoder step are connected in a DenseNet fashion to subsequent transformer modules in the same step by being concatenated with the steps input and all transformer module outputs up to that point. Since all communication across frames and thus any contextual information transfers between frames in transformer modules, I have these throughout the network. A test with just the 3x1 kernel convolutions and no transformer modules led to poor accuracy, although I need to verify if its the attention helping or if its the 1d convolutions.

I have a local version that is set up for Google Cloud, but not sure how I want to set the branch up yet. TPU version is set up but might not work, have been unable to successfully aquire TPUs as part of an AI Platform job and don't want to waste any more money on that.

This fork makes use of vocal augmentations. The dataset takes an instrumental path, an optional second instrumental path if you have a dataset split across multiple locations, a vocal path, and a pair path. The instrumental path expects only instrumental songs in npz files in the folder specified (although technically it doesn't matter so be careful; even with a massive dataset a few really bad examples can ruin learning - just learned this the hard way and earned myself an evening of sifting through countless audio files...). The dataset's __getitem__ method will check to see if a Y value exists in the npz file; instrumental npzs will be expected to only have an 'X' and a 'c' key defined. If a 'Y' key does not exist, it will treat it as a file that should be augmented. This will cause the dataset to randomly select a vocal file, augment the vocal spectrogram, and then add the vocal spectrogram to the instrumental track and select the largest of either the instrumental max magnitude, vocal max magnitude, or the max magnitude of that slice. There is also a chance that vocals will be randomly added to pair tracks, so the vocal version will get an extra layer of vocals added to it and a new divisor selected if need be for normalization.

## References
- [1] Jansson et al., "Singing Voice Separation with Deep U-Net Convolutional Networks", https://ismir2017.smcnus.org/wp-content/uploads/2017/10/171_Paper.pdf
- [2] Takahashi et al., "Multi-scale Multi-band DenseNets for Audio Source Separation", https://arxiv.org/pdf/1706.09588.pdf
- [3] Takahashi et al., "MMDENSELSTM: AN EFFICIENT COMBINATION OF CONVOLUTIONAL AND RECURRENT NEURAL NETWORKS FOR AUDIO SOURCE SEPARATION", https://arxiv.org/pdf/1805.02410.pdf
- [4] Liutkus et al., "The 2016 Signal Separation Evaluation Campaign", Latent Variable Analysis and Signal Separation - 12th International Conference
- [5] So, Liang, and Le, "The Evolved Transformer", https://arxiv.org/pdf/1901.11117.pdf
- [6] He et al., "RealFormer: Transformer Likes Residual Attention", https://arxiv.org/pdf/2012.11747.pdf